# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "3d14c19d-21db-49af-873f-1621ee27460c",
# META       "default_lakehouse_name": "lh_DS_BankChurn",
# META       "default_lakehouse_workspace_id": "e82dfb36-dba0-483b-8860-67b2a08d0487",
# META       "known_lakehouses": [
# META         {
# META           "id": "3d14c19d-21db-49af-873f-1621ee27460c"
# META         }
# META       ]
# META     }
# META   }
# META }

# CELL ********************

import os, requests
import itertools

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
from matplotlib import rc, rcParams
import seaborn as sns
sns.set_theme(style="whitegrid", palette="tab10", rc = {'figure.figsize':(9,6)})

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# **Download dataset and upload to lakehouse**

# CELL ********************

IS_CUSTOM_DATA = False  # if TRUE, dataset has to be uploaded manually

DATA_ROOT = "/lakehouse/default"
DATA_FOLDER = "Files/BankChurn"  # folder with data files
DATA_FILE = "churn.csv"  # data file name

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

if not IS_CUSTOM_DATA:
# Download demo data files into lakehouse if not exist
    remote_url = "https://synapseaisolutionsa.z13.web.core.windows.net/data/bankcustomerchurn"
    file_list = [DATA_FILE]
    download_path = f"{DATA_ROOT}/{DATA_FOLDER}"

    if not os.path.exists("/lakehouse/default"):
        raise FileNotFoundError(
            "Default lakehouse not found, please add a lakehouse and restart the session."
        )
    os.makedirs(download_path, exist_ok=True)
    for fname in file_list:
        if not os.path.exists(f"{download_path}/{fname}"):
            r = requests.get(f"{remote_url}/{fname}", timeout=30)
            with open(f"{download_path}/{fname}", "wb") as f:
                f.write(r.content)
    print("Downloaded demo data files into lakehouse.")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# **Read raw data from the lakehouse**

# CELL ********************

df = (
    spark.read.option("header", True)
    .option("inferSchema", True)
    .csv("Files/BankChurn/churn.csv")
    .cache()
)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

df.explain()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

df.describe()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(df.show(10, False))

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

df.summary().show()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(df, summary=True)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# **Create a pandas DataFrame from the dataset**

# CELL ********************

df_pandas = df.toPandas()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

df_pandas.info()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

df_pandas.describe()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(df_pandas, summary=True)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# **Use Data Wrangler to perform initial data cleaning**

# CELL ********************

# Modified version of code generated by Data Wrangler 
# Modification is to add in-place=True to each step

# Define a new function that include all above Data Wrangler operations
def clean_data(df_pandas):
    # Drop rows with missing data across all columns
    df_pandas.dropna(inplace=True)
    # Drop duplicate rows in columns: 'RowNumber', 'CustomerId'
    df_pandas.drop_duplicates(subset=['RowNumber', 'CustomerId'], inplace=True)
    # Drop columns: 'RowNumber', 'CustomerId', 'Surname'
    df_pandas.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)
    return df_pandas

df_pandas_clean = clean_data(df_pandas.copy())
df_pandas_clean.head()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# **Exploratory Data Analysis (EDA)**

# CELL ********************

# Determine categorical, numerical, and target attributes

# Determine the dependent (target) attribute
dependent_variable_name = "Exited"
print(dependent_variable_name)

# Determine the categorical attributes
categorical_variables = [col for col in df_pandas_clean.columns if col in "O"
                                                                   or df_pandas_clean[col].nunique() <=5
                                                                   and col not in "Exited"
                                                                   ]
print(categorical_variables)

# Determine the numerical attributes
numeric_variables = [col for col in df_pandas_clean.columns if df_pandas_clean[col].dtype != "object"
                                                               and df_pandas_clean[col].nunique() >5
                                                               ]
print(numeric_variables)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# The five-number summary

df_num_cols = df_pandas_clean[numeric_variables]
sns.set(font_scale = 0.7) 

fig, axes = plt.subplots(nrows=2, ncols=3, gridspec_kw=dict(hspace=0.3), figsize=(17,8))
fig.tight_layout()

for ax, col in zip(axes.flatten(), df_num_cols.columns):
    sns.boxplot(x=df_num_cols[col], color='green', ax=ax)

fig.delaxes(axes[1,2])

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Distribution of exited and nonexited customers

attr_list = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'NumOfProducts', 'Tenure']
df_pandas_clean['Exited'] = df_pandas_clean['Exited'].astype(str)

fig, axarr = plt.subplots(2, 3, figsize=(15,4))

for ind, item in enumerate (attr_list):
    sns.countplot(x=item, hue='Exited', data=df_pandas_clean, ax=axarr[ind%2][ind//2])

fig.subplots_adjust(hspace=0.7)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Distribution of numerical attributes

df_num_cols = df_pandas_clean[numeric_variables]
columns = df_num_cols.columns[:len(df_num_cols.columns)]

fig = plt.figure()
fig.set_size_inches(18, 8)
length = len(columns)

for i,j in itertools.zip_longest(columns, range(length)):
    plt.subplot((length // 2), 3, j+1)
    plt.subplots_adjust(wspace=0.2, hspace=0.5)
    df_num_cols[i].hist(bins=20, edgecolor='black')
    plt.title(i)

plt.show()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Perform feature engineering

df_pandas_clean['Tenure'] = df_pandas_clean['Tenure'].astype(int)
df_pandas_clean["NewTenure"] = df_pandas_clean["Tenure"] / df_pandas_clean["Age"]
df_pandas_clean["NewCreditsScore"] = pd.qcut(df_pandas_clean['CreditScore'], 6, labels=[1,2,3,4,5,6])
df_pandas_clean["NewAgeScore"] = pd.qcut(df_pandas_clean['Age'], 8, labels=[1,2,3,4,5,6,7,8])
df_pandas_clean["NewBalanceScore"] = pd.qcut(df_pandas_clean['Balance'].rank(method="first"), 5, labels=[1,2,3,4,5])
df_pandas_clean["NewEstSalaryScore"] = pd.qcut(df_pandas_clean['EstimatedSalary'], 10, labels=[1,2,3,4,5,6,7,8,9,10])

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

# Use Data Wrangler to perform one-hot encoding

def clean_data(df_pandas_clean):
    # One-hot encode columns: 'Geography', 'Gender'
    df_pandas_clean = pd.get_dummies(df_pandas_clean, columns=['Geography', 'Gender'])
    return df_pandas_clean
 
df_pandas_clean = clean_data(df_pandas_clean.copy())
df_pandas_clean.head()

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# MARKDOWN ********************

# **Create a delta table for the cleaned data**

# CELL ********************

table_name = "churn_clean"

# Create Spark DataFrame from pandas
sparkDF = spark.createDataFrame(df_pandas_clean) 
sparkDF.write.mode("overwrite").format("delta").save(f"Tables/dbo/{table_name}")
print(f"Spark dataframe saved to delta table: {table_name}")

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

display(sparkDF, summary=True)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
