# Data science end-to-end scenario
This project present a complete end-to-end scenario in the Fabric data science to explore, clean, and transform a dataset that contains the churn status of 10,000 bank customers. 
It covers each step, from:
- Data ingestion
- Data cleaning
- Data preparation
to
- Machine learning model training
- Insight generation
and then cover consumption of those insights with visualization tools - for example, Power BI.

# Architecture
The following activities are covered in the project:
1.	Use the Fabric notebooks for data science scenarios
2.	Use Apache Spark to ingest data into a Fabric lakehouse
3.	Load existing data from the lakehouse delta tables
4.	Use Apache Spark and Python-based tools to clean and transform data
5.	Create experiments and runs to train different machine learning models
6.	Use MLflow and the Fabric UI to register and track trained models
7.	Run scoring at scale, and save predictions and inference results to the lakehouse
8.	Use DirectLake to visualize predictions in Power BI Architecture
